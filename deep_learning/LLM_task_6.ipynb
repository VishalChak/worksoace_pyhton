{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731c8aa9-bd50-4446-af2d-b905db2e515b",
   "metadata": {},
   "source": [
    "### Text Generation (a.k.a. Language Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd7a5cc-7567-488d-951c-0accb7fdcf6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"If you are interested in learing more about data science, I can teach you how to get started with Linux without installing any additional dependencies (i.e. a separate tool for windows so you don't have to run into a Linux issue\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from transformers import pipeline\n",
    "\n",
    "# Specify the model\n",
    "model = \"gpt2\"\n",
    "\n",
    "# Specify the task\n",
    "task = \"text-generation\"\n",
    "\n",
    "# Instantiate pipeline\n",
    "generator = pipeline(model = model, task = task, max_new_tokens = 30)\n",
    "\n",
    "# Specify input text\n",
    "input_text = \"If you are interested in learing more about data science, I can teach you how to\"\n",
    "\n",
    "# Perform text generation and store the results\n",
    "output = generator(input_text)\n",
    "\n",
    "# Return the results\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe460ea7-bd98-4ea6-80f3-cf2a8f23692b",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "\n",
    "#### There are generally two types of question answering tasks:\n",
    "\n",
    "##### Extractive (i.e. context-dependent):\n",
    "Where the user describes a situation to the model in the question/prompt and ask the model to generate a response, given that provided information. In this scenario, the model picks the relevant parts of the information from the prompt and returns the results.\n",
    "\n",
    "##### Abstractive (i.e. context-independent):\n",
    "Where the user asks a question from the model, without providing any context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5aa89b-a1b2-493b-844c-13ade357ca7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.3341, start: 65, end: 92, answer: natural language processing\n"
     ]
    }
   ],
   "source": [
    "# Specify model\n",
    "model = 'distilbert-base-cased-distilled-squad'\n",
    "# Instantiate pipeline\n",
    "answerer = pipeline(model = model, task=\"question-answering\")\n",
    "\n",
    "# Specify question and context\n",
    "question = \"What does NLP stand for?\"\n",
    "context = \"Today we are talking about machine learning and specifically the natural language processing, which enables computers to understand, process and generate languages\"\n",
    "\n",
    "# Generate predictions\n",
    "preds = answerer(\n",
    "    question = question,\n",
    "    context = context,\n",
    ")\n",
    "\n",
    "# Return results\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3642bd-a826-41d6-9cf4-3e3447e493bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1a7d1279fd429594268c3c8d1fb107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6c1d7762b0442e9fd3f56b98a67e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c0ed0e36d347b7855eaa493fda792b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f052a05d4a48ccad95613d32b53612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692e924609744f1c94ca204359c6c742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117a99cbfaed4555a39ceeb0fea9b9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.04662749171257019,\n",
       " 'start': 65,\n",
       " 'end': 92,\n",
       " 'answer': 'natural language processing'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify model\n",
    "model = \"deepset/roberta-base-squad2\"\n",
    "# Specify task\n",
    "task = \"question-answering\"\n",
    "\n",
    "# Instantiate pipeline\n",
    "answerer = pipeline(task = task, model = model, tokenizer = model)\n",
    "\n",
    "# Specify input\n",
    "qa_input = {\n",
    "    'question': 'What does NLP stand for?',\n",
    "    'context': 'Today we are talking about machine learning and specifically the natural language processing, which enables computers to understand, process and generate languages'\n",
    "}\n",
    "\n",
    "# Generate predictions\n",
    "output = answerer(qa_input)\n",
    "\n",
    "# Return results\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab80def-88d8-4550-8e26-beb413638cfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sentiment Analysis — Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "891e24e9-a5b0-42b8-8236-75e43968e62f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8548845052719116}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify pre-trained model to use\n",
    "model = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "# Specify task\n",
    "task = 'sentiment-analysis'\n",
    "\n",
    "# Text to be analyzed\n",
    "input_text = 'Performing NLP tasks using HuggingFace pipeline is super easy!'\n",
    "\n",
    "# Instantiate pipeline\n",
    "analyzer = pipeline(task, model = model)\n",
    "\n",
    "# Store the output of the analysis\n",
    "output = analyzer(input_text)\n",
    "\n",
    "# Return output\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f28e8-2dd0-4ea5-b481-6eb1457ab2e4",
   "metadata": {},
   "source": [
    "### Text Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e198ebf-b2c7-4e17-bbb8-a80b636409ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'sequence': 'This is a tutorial about using pre-trained models through HuggingFace', 'labels': ['education', 'business', 'music', 'sports', 'politics'], 'scores': [0.40113699436187744, 0.2170693427324295, 0.14547252655029297, 0.14507530629634857, 0.09124579280614853]}\n",
      "1 {'sequence': 'This is a tutorial about using pre-trained models through HuggingFace', 'labels': ['education', 'business', 'music', 'sports', 'politics'], 'scores': [0.40113699436187744, 0.2170693427324295, 0.14547252655029297, 0.14507530629634857, 0.09124579280614853]}\n",
      "2 {'sequence': 'This is a tutorial about using pre-trained models through HuggingFace', 'labels': ['education', 'business', 'music', 'sports', 'politics'], 'scores': [0.40113699436187744, 0.2170693427324295, 0.14547252655029297, 0.14507530629634857, 0.09124579280614853]}\n",
      "3 {'sequence': 'This is a tutorial about using pre-trained models through HuggingFace', 'labels': ['education', 'business', 'music', 'sports', 'politics'], 'scores': [0.40113699436187744, 0.2170693427324295, 0.14547252655029297, 0.14507530629634857, 0.09124579280614853]}\n",
      "4 {'sequence': 'This is a tutorial about using pre-trained models through HuggingFace', 'labels': ['education', 'business', 'music', 'sports', 'politics'], 'scores': [0.40113699436187744, 0.2170693427324295, 0.14547252655029297, 0.14507530629634857, 0.09124579280614853]}\n"
     ]
    }
   ],
   "source": [
    "# Specify model\n",
    "model = 'facebook/bart-large-mnli'\n",
    "# Specify Task\n",
    "task = 'zero-shot-classification'\n",
    "\n",
    "# Specify input text\n",
    "input_text = 'This is a tutorial about using pre-trained models through HuggingFace'\n",
    "\n",
    "# Identify the classes/categories/labels\n",
    "labels = ['business', 'sports', 'education', 'politics', 'music']\n",
    "for i in range(5):\n",
    "# Instantiate pipeline\n",
    "    classifier = pipeline(task, model = model, device = 0 )\n",
    "\n",
    "    # Store the output of the analysis\n",
    "    output = classifier(input_text, candidate_labels = labels)\n",
    "\n",
    "    # Return output\n",
    "    print(i , output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00639198-c184-4a72-948f-a8ef674611ec",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f5ec2ee-b42e-4f1a-ad42-c68714be3dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a160d12d00754dc7adf4ef1b001b07b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4f50119a674abfabe7107dc015801b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64794d0d8f584ae1bc9bf64bf1c8fd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdbbae9c4d34c3596634938e58d3110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chak/anaconda3/envs/hugging/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 200, but you input_length is only 82. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'text summarization is the task of automatically summarizing textual input . using a model can save human time in situations where humans read incoming text .'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify model and tokenizer\n",
    "model = \"t5-base\"\n",
    "tokenizer = \"t5-base\"\n",
    "# Specify task\n",
    "task = \"summarization\"\n",
    "\n",
    "# Specify input text\n",
    "input_text = \"Text summarization is the task of automatically summarizing textual input, while still conveying the main points and gist of the incoming text. One example of the business intuition behind the need for such summarization models is the situations where humans read incoming text communications (e.g. customer emails) and using a summarization model can save human time. \"\n",
    "\n",
    "# Instantiate pipeline\n",
    "summarizer = pipeline(task = task, model = model, tokenizer = tokenizer)\n",
    "\n",
    "# Summarize and store results\n",
    "output = summarizer(input_text)\n",
    "\n",
    "# Return output\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e757b-099e-484a-a54f-ae131e0e78ae",
   "metadata": {},
   "source": [
    "### Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77ce4221-b406-4109-943a-50d1fd5007f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Il s'agit d'un poste sur Medium sur diverses tâches de NLP utilisant Hugging Face.\"}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify prefix\n",
    "original_language = 'English'\n",
    "target_language = 'French'\n",
    "prefix = f\"translate {original_language} to {target_language}: \"\n",
    "# Specify input text\n",
    "input_text = f\"{prefix}This is a post on Medium about various NLP tasks using Hugging Face.\"\n",
    "\n",
    "# Specify model\n",
    "model = \"t5-base\"\n",
    "\n",
    "# Specify task\n",
    "task = \"translation\"\n",
    "\n",
    "# Instantiate pipeline\n",
    "translator = pipeline(task = task, model = model)\n",
    "\n",
    "# Perform translation and store the output\n",
    "output = translator(input_text)\n",
    "\n",
    "# Return output\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558d64a-2724-4734-8b8a-6f4b0dcdee14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
